{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d830ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries. Ensure all dependencies are defined in requirements.txt and installed in your virtual environment.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (\n",
    "    row_number, lit, when, col, to_date, concat_ws, current_timestamp\n",
    "    , round as pyspark_round, udf\n",
    ")\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DecimalType, DateType\n",
    "import requests\n",
    "import time\n",
    "import pyodbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0ee37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User to set constant pipeline variables which should be altered when a new environment \n",
    "# or new file is to be ingested\n",
    "\n",
    "# Documented at the top of the script for ease of updating / maintenance\n",
    "# Recommended in the future that seperate config be used to manage variables\n",
    "# Delivery requirement in this use case was a single PySpark notebook\n",
    "\n",
    "# Path to the JDBC JAR file. Update to your JDBC JAR path\n",
    "JDBC_DRIVER_PATH = \"/Users/abbywalker/Documents/coding/currency_conversion_proj/jars/mssql-jdbc-12.10.1.jre11.jar\"\n",
    "\n",
    "#Â Path to code repository\n",
    "CODE_REPO = os.getcwd()\n",
    "\n",
    "# Sales data file name\n",
    "SALES_DATA_FILENAME = 'sales_data 2'\n",
    "SALES_DATA_FILETYPE = 'csv'\n",
    "\n",
    "# Product reference file name\n",
    "PRODUCT_REF_FILENAME = 'product_reference 2'\n",
    "PRODUCT_REF_FILETYPE = 'csv'\n",
    "\n",
    "# Set Azure SQL database variables\n",
    "SERVER_NAME = \"applab-tech-interview-sqls.database.windows.net\"\n",
    "DB_NAME = \"applab-prod-sql-db\"\n",
    "\n",
    "# Percentage of records with poor data quality which should trigger the job to abort\n",
    "DATA_QUALITY_FAIL_RATE = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3a4eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant variables set which utilise variables set above\n",
    "\n",
    "# Landing zone for data ingestion (folder path to where the source data is)\n",
    "LANDING_ZONE_DIR = CODE_REPO + '/data/'\n",
    "\n",
    "# Setting the file paths to ingest the data\n",
    "SALES_DATA_INGEST_PATH = f\"{LANDING_ZONE_DIR}{SALES_DATA_FILENAME}.{SALES_DATA_FILETYPE}\"\n",
    "PRODUCT_REF_INGEST_PATH = f\"{LANDING_ZONE_DIR}{PRODUCT_REF_FILENAME}.{PRODUCT_REF_FILETYPE}\"\n",
    "\n",
    "# Script path to set-up database tables\n",
    "CREATE_DB_TABLES = CODE_REPO + '/CREATE_tables.sql'\n",
    "\n",
    "# Load the .env file consisting of the Azure SQL database username and password\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the credentials\n",
    "DB_USERNAME = os.getenv(\"DB_USERNAME\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "# Build JDBC URL\n",
    "JDBC_URL = (\n",
    "        f\"jdbc:sqlserver://{SERVER_NAME}:1433;\"\n",
    "        f\"databaseName={DB_NAME};\"\n",
    "        \"encrypt=true;\"\n",
    "        \"trustServerCertificate=false;\"\n",
    "        \"hostNameInCertificate=*.database.windows.net;\"\n",
    "        \"loginTimeout=30;\"\n",
    ")\n",
    "\n",
    "CONNECTION_PROPERTIES = {\n",
    "        \"user\": f\"{DB_USERNAME}@{SERVER_NAME}\",\n",
    "        \"password\": DB_PASSWORD,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "sales_schema_dict = {\n",
    "        \"OrderID\": [\"order_id\", \"string\"],\n",
    "        \"ProductID\": [\"product_id\", \"string\"],\n",
    "        \"SaleAmount\": [\"sale_amount\", \"decimal\", (18, 2)],\n",
    "        \"OrderDate\": [\"order_date\", \"date\"],\n",
    "        \"Region\": [\"region\", \"string\"],\n",
    "        \"CustomerID\": [\"customer_id\", \"string\"],\n",
    "        \"Discount\": [\"discount\", \"decimal\", (10, 2)],\n",
    "        \"Currency\": [\"currency\", \"string\"]\n",
    "    }\n",
    "\n",
    "product_ref_schema_dict = {\n",
    "        \"ProductID\": [\"product_id\", \"string\"],\n",
    "        \"ProductName\": [\"product_name\", \"string\"],\n",
    "        \"Category\": [\"product_reference\", \"string\"]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf38345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(name=\"production_logger\", level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Sets up and returns a logger with a stream handler and formatter.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Create a pipeline handler and set its level\n",
    "    pipeline_handler = logging.StreamHandler()\n",
    "    pipeline_handler.setLevel(level)\n",
    "\n",
    "    # Create a formatter and set it for the handler\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    pipeline_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the handler to the logger if not already present\n",
    "    if not logger.hasHandlers():\n",
    "        logger.addHandler(pipeline_handler)\n",
    "\n",
    "    logger.info(\"Logger initialised and ready to use.\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416999d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_spark_session(logger, app_name=\"ETLPipeline\", jdbc_driver_path=JDBC_DRIVER_PATH):\n",
    "    \"\"\"\n",
    "    Checks if a SparkSession exists. If not, creates and returns a new SparkSession.\n",
    "    \"\"\"\n",
    "    spark = SparkSession._instantiatedSession\n",
    "    if spark is None:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.jars\", jdbc_driver_path) \\\n",
    "            .getOrCreate()\n",
    "        logger.info(\"Initialised Spark Session\")\n",
    "    else:\n",
    "        logger.info(\"Spark Session already exists\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27003a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_tables_exist(spark\n",
    "                        , SERVER_NAME\n",
    "                        , DB_NAME\n",
    "                        , DB_USERNAME\n",
    "                        , DB_PASSWORD\n",
    "                        , target_tables\n",
    "                        , CREATE_DB_TABLES\n",
    "                        , logger\n",
    "                        , CONNECTION_PROPERTIES=CONNECTION_PROPERTIES\n",
    "                        , JDBC_URL=JDBC_URL):\n",
    "    \"\"\"\n",
    "    Checks if the target tables exist in the Azure SQL Database using PySpark.\n",
    "    If any table does not exist, executes the SQL script at CREATE_DB_TABLES to create them using PyODBC.\n",
    "    Retries the initial connection twice with a 15 second pause if needed.\n",
    "    If any of the tables already exist, skips creation and continues.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retry logic for initial connection\n",
    "    max_attempts = 2\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            tables_df = spark.read.jdbc(\n",
    "                url=JDBC_URL,\n",
    "                table=\"INFORMATION_SCHEMA.TABLES\",\n",
    "                properties=CONNECTION_PROPERTIES\n",
    "            )\n",
    "            logger.info(\"Successfully connected to Azure SQL Database.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to Azure SQL Database (attempt {attempt + 1}): {e}\")\n",
    "            attempt += 1\n",
    "            if attempt < max_attempts:\n",
    "                logger.info(\"Retrying in 15 seconds...\")\n",
    "                time.sleep(15)\n",
    "            else:\n",
    "                logger.error(\"All connection attempts failed.\")\n",
    "                return\n",
    "\n",
    "    # Normalize table names for comparison (lowercase, strip whitespace)\n",
    "    existing_tables = [row['TABLE_NAME'].strip().lower() for row in tables_df.collect()]\n",
    "    normalized_targets = [t.strip().lower() for t in target_tables]\n",
    "    missing_tables = [t for t in target_tables if t.strip().lower() not in existing_tables]\n",
    "\n",
    "    if not missing_tables:\n",
    "        logger.info(\"All target tables exist in the database. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Missing tables detected: {missing_tables}. Executing SQL script to create missing tables using PyODBC.\")\n",
    "    # Read SQL script\n",
    "    with open(CREATE_DB_TABLES, 'r') as file:\n",
    "        sql_script = file.read()\n",
    "    # Split into statements (assuming ';' as separator)\n",
    "    sql_statements = [stmt.strip() for stmt in sql_script.split(';') if stmt.strip()]\n",
    "    # Build PyODBC connection string (using ODBC Driver 18)\n",
    "    odbc_conn_str = (\n",
    "        f\"DRIVER={{ODBC Driver 18 for SQL Server}};\"\n",
    "        f\"SERVER={SERVER_NAME};\"\n",
    "        f\"DATABASE={DB_NAME};\"\n",
    "        f\"UID={DB_USERNAME};\"\n",
    "        f\"PWD={DB_PASSWORD};\"\n",
    "        \"Encrypt=yes;\"\n",
    "        \"TrustServerCertificate=no;\"\n",
    "        \"Connection Timeout=30;\"\n",
    "    )\n",
    "    try:\n",
    "        with pyodbc.connect(odbc_conn_str) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            for table in missing_tables:\n",
    "                # Find the statement(s) that create this table\n",
    "                relevant_statements = [stmt for stmt in sql_statements if table in stmt]\n",
    "                if not relevant_statements:\n",
    "                    logger.warning(f\"No CREATE statement found for table: {table}\")\n",
    "                    continue\n",
    "                for statement in relevant_statements:\n",
    "                    try:\n",
    "                        cursor.execute(statement)\n",
    "                        conn.commit()\n",
    "                        logger.info(f\"Executed SQL statement for table {table}: {statement}\")\n",
    "                    except Exception as e:\n",
    "                        # If table already exists, skip error and continue\n",
    "                        if \"already exists\" in str(e).lower():\n",
    "                            logger.info(f\"Table {table} already exists. Skipping creation.\")\n",
    "                            continue\n",
    "                        logger.error(f\"Failed to execute SQL statement for table {table}: {statement}\\nError: {e}\")\n",
    "                        raise RuntimeError(\"Force stop due to error.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect or execute SQL script using PyODBC: {e}\")\n",
    "        raise RuntimeError(\"Force stop due to error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa881a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_setup():\n",
    "    # Check the database username and password exist following parameter setting\n",
    "    # If there is an issue, update the file then restart the Python kernal before running\n",
    "    assert DB_USERNAME, \"Missing DB_USERNAME\" \n",
    "    assert DB_PASSWORD, \"Missing DB_PASSWORD\" \n",
    "\n",
    "    # Set up logger\n",
    "    logger = setup_logger()\n",
    "\n",
    "    # Create or get Spark session\n",
    "    spark = get_or_create_spark_session(logger)\n",
    "\n",
    "    # Define target tables to check/create\n",
    "    target_tables = [\"product_ref\", \"orders\", \"order_product\"]\n",
    "\n",
    "    # Ensure tables exist in the Azure SQL Database\n",
    "    ensure_tables_exist(\n",
    "        spark,\n",
    "        SERVER_NAME,\n",
    "        DB_NAME,\n",
    "        DB_USERNAME,\n",
    "        DB_PASSWORD,\n",
    "        target_tables,\n",
    "        CREATE_DB_TABLES,\n",
    "        logger\n",
    "    )\n",
    "\n",
    "    logger.info(\"Pipeline setup complete.\")\n",
    "    return spark, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2407d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns_for_is_null(df, logger, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Adds a binary 'is_null' column to the PySpark DataFrame, set to 1 if any column (except excluded) is null in the row, else 0.\n",
    "    \"\"\"\n",
    "\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "    columns_to_check = [c for c in df.columns if c not in exclude_columns]\n",
    "    condition = None\n",
    "    for c in columns_to_check:\n",
    "        col_is_null = col(c).isNull()\n",
    "        condition = col_is_null if condition is None else (condition | col_is_null)\n",
    "    if condition is None:\n",
    "        # No columns to check, set is_null to 0\n",
    "        return df.withColumn(\"is_null\", lit(0))\n",
    "    logger.info(\"Flagged all rows with NULLs which violate exceptions.\")\n",
    "    return df.withColumn(\"is_null\", when(condition, lit(1)).otherwise(lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d63f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_duplicate_rows(df, logger):\n",
    "    \"\"\"\n",
    "    Adds a binary 'is_dup_row' column to the PySpark DataFrame.\n",
    "    'is_dup_row' is 1 for duplicate rows (excluding the first occurrence), 0 otherwise.\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy([c for c in df.columns]).orderBy(lit(1))\n",
    "    df_with_rownum = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "    df_with_isdup = df_with_rownum.withColumn(\"is_dup_row\", when(col(\"row_num\") > 1, lit(1)).otherwise(lit(0)))\n",
    "    logger.info(\"Flagged all rows which are duplicates.\")\n",
    "    return df_with_isdup.drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c675afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_csv(spark, file_path, file_type, logger, load_new_file=True):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file into a PySpark DataFrame.\n",
    "    Only CSV files are supported. Raises an error for other file types.\n",
    "    Logs info and error messages using the provided logger.\n",
    "    Returns the DataFrame or None if load_new_file is False.\n",
    "    \"\"\"\n",
    "    if not load_new_file:\n",
    "        logger.info(\"Skipping loading of new file as per input flag.\")\n",
    "        return None\n",
    "\n",
    "    if file_type.lower() != 'csv':\n",
    "        logger.error(f\"Unsupported file type: {file_type}. Only 'csv' is supported.\")\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}. Only 'csv' is supported.\")\n",
    "\n",
    "    try:\n",
    "        df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "        logger.info(f\"Loaded data from {file_path} as CSV.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data from {file_path}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed39f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(\n",
    "    spark,\n",
    "    logger,\n",
    "    CONNECTION_PROPERTIES=CONNECTION_PROPERTIES,\n",
    "    JDBC_URL=JDBC_URL,\n",
    "    DATA_QUALITY_FAIL_RATE = DATA_QUALITY_FAIL_RATE,\n",
    "    load_product_ref=True,\n",
    "    load_sales_data=True,\n",
    "    product_ref_path=PRODUCT_REF_INGEST_PATH,\n",
    "    product_ref_type=PRODUCT_REF_FILETYPE,\n",
    "    sales_data_path=SALES_DATA_INGEST_PATH,\n",
    "    sales_data_type=SALES_DATA_FILETYPE\n",
    "):\n",
    "    \"\"\"\n",
    "    ETL pipeline for sales data with data quality checks and logging.\n",
    "    Steps:\n",
    "    1. Load product_ref and sales_data if flags are True. If product_ref \n",
    "        data is not loaded then retrieve from the database.\n",
    "    2. Process both through is_null and duplicate row functions.\n",
    "    3. Join sales_data to product_ref and flag orphaned product_ids.\n",
    "    4. Evaluate if >5% of records have poor data quality (nulls, duplicates, or orphaned).\n",
    "    5. Log all steps and catch common errors.\n",
    "    6. Split joined_df into validated_df and quarantine_df, write quarantine_df to quarantine_record table, return validated_df.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initial data validation\n",
    "    try:\n",
    "        product_ref_df = None\n",
    "        if load_product_ref:\n",
    "            logger.info(\"Loading product_ref data from file...\")\n",
    "            product_ref_df = load_data_csv(spark, product_ref_path, product_ref_type, logger)\n",
    "            product_ref_df = check_columns_for_is_null(product_ref_df, logger)\n",
    "            product_ref_df = mark_duplicate_rows(product_ref_df, logger)\n",
    "            product_ref_validate_df = product_ref_df.select(col(\"ProductID\").alias(\"ref_product_id\"))\n",
    "            logger.info(\"product_ref data loaded and processed from file.\")\n",
    "        else:\n",
    "            logger.info(\"Loading product_ref data from Azure SQL Database...\")\n",
    "            product_ref_df = spark.read.jdbc(\n",
    "                url=JDBC_URL,\n",
    "                table=\"product_ref\",\n",
    "                properties=CONNECTION_PROPERTIES\n",
    "            )\n",
    "            product_ref_df = check_columns_for_is_null(product_ref_df, logger)\n",
    "            product_ref_df = mark_duplicate_rows(product_ref_df, logger)\n",
    "            product_ref_validate_df = product_ref_df.select(col(\"product_id\").alias(\"ref_product_id\"))\n",
    "            logger.info(\"product_ref data loaded and processed from Azure SQL Database.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading or processing product_ref: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        sales_data_df = None\n",
    "        if load_sales_data:\n",
    "            logger.info(\"Loading sales_data...\")\n",
    "            sales_data_df = load_data_csv(spark, sales_data_path, sales_data_type, logger)\n",
    "            sales_data_df = check_columns_for_is_null(sales_data_df, logger, exclude_columns='Discount')\n",
    "            sales_data_df = mark_duplicate_rows(sales_data_df, logger)\n",
    "            logger.info(\"sales_data loaded and processed.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading or processing sales_data: {e}\")\n",
    "        return\n",
    "\n",
    "    if product_ref_df is not None and sales_data_df is not None:\n",
    "        try:\n",
    "            logger.info(\"Joining sales_data to product_ref to check for orphaned product_ids...\")\n",
    "            joined_df = sales_data_df.join(\n",
    "                product_ref_validate_df,\n",
    "                sales_data_df[\"ProductID\"] == col(\"ref_product_id\"),\n",
    "                how=\"left\"\n",
    "            )\n",
    "            # Orphaned if ref_product_id is null\n",
    "            joined_df = joined_df.withColumn(\n",
    "                \"is_orphaned_product_id\",\n",
    "                when(col(\"ref_product_id\").isNull(), lit(1)).otherwise(lit(0))\n",
    "            )\n",
    "            logger.info(\"Join complete. Flagged orphaned product_ids.\")\n",
    "\n",
    "            # Data quality evaluation\n",
    "            total_rows = joined_df.count()\n",
    "            quarantine_df = joined_df.filter(\n",
    "                (col(\"is_null\") == 1) | (col(\"is_dup_row\") == 1) | (col(\"is_orphaned_product_id\") == 1)\n",
    "            )\n",
    "            validated_df = joined_df.filter(\n",
    "                (col(\"is_null\") == 0) & (col(\"is_dup_row\") == 0) & (col(\"is_orphaned_product_id\") == 0)\n",
    "            )\n",
    "            poor_quality_rows = quarantine_df.count()\n",
    "            percent_poor_quality = (poor_quality_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "\n",
    "            logger.info(f\"Total rows: {total_rows}, Poor quality rows: {poor_quality_rows} ({percent_poor_quality:.2f}%)\")\n",
    "\n",
    "            # Write the quarantine_df to the database for auditing\n",
    "            quarantine_df.write.jdbc(\n",
    "                url=JDBC_URL,\n",
    "                table=\"quarantine_data\",\n",
    "                mode=\"overwrite\", # in the future this should be an append\n",
    "                properties=CONNECTION_PROPERTIES\n",
    "            )\n",
    "\n",
    "            if percent_poor_quality > DATA_QUALITY_FAIL_RATE:\n",
    "                logger.error(f\"More than {DATA_QUALITY_FAIL_RATE}% of records have poor data quality. Stopping the job\")\n",
    "                raise RuntimeError(\"Force stop due to error.\")\n",
    "            else:\n",
    "                logger.info(f\"Data quality is within acceptable limits of {DATA_QUALITY_FAIL_RATE}%.\")\n",
    "\n",
    "            return validated_df, product_ref_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during join or data quality evaluation: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        logger.warning(\"Skipping join and data quality evaluation as one or both datasets were not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "442e10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF that tries multiple formats for parsing dates safely\n",
    "def parse_date_safe(date_str):\n",
    "    \"\"\"\n",
    "    Attempts to parse a date string using multiple common formats.\n",
    "    Returns a date object if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    if date_str is None:\n",
    "        return None\n",
    "    for fmt in (\"%d/%m/%Y\", \"%d-%m-%Y\", \"%Y-%d-%m\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt).date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None  # All formats failed\n",
    "\n",
    "# Register UDF\n",
    "parse_date_udf = udf(parse_date_safe, DateType())\n",
    "\n",
    "def transform_schema(df, schema_dict):\n",
    "    \"\"\"\n",
    "    Transforms the input DataFrame columns according to the schema_dict.\n",
    "    Handles explicit types: decimal, date, and string (no cast needed for string).\n",
    "    After transformation, keeps only the columns mapped in schema_dict (using their new/target names).\n",
    "    Drops all other columns not in the schema_dict.\n",
    "    \"\"\"\n",
    "    for raw_col, mapping in schema_dict.items():\n",
    "        target_col = mapping[0]\n",
    "        if len(mapping) > 1:\n",
    "            dtype = mapping[1]\n",
    "            if dtype == \"decimal\":\n",
    "                precision, scale = mapping[2]\n",
    "                df = df.withColumn(target_col, col(raw_col).cast(DecimalType(precision, scale)))\n",
    "            elif dtype == \"date\":\n",
    "                df = df.withColumn(target_col, parse_date_udf(col(raw_col)))\n",
    "            else:\n",
    "                df = df.withColumn(target_col, col(raw_col))\n",
    "        else:\n",
    "            df = df.withColumn(target_col, col(raw_col))\n",
    "\n",
    "    # Only keep the target columns (new names from schema_dict)\n",
    "    target_cols = [mapping[0] for mapping in schema_dict.values()]\n",
    "    df = df.select(*target_cols)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4c77ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "def fetch_usd_exchange_rates(spark, logger, currency_list, base_url='https://open.er-api.com/v6/latest/'):\n",
    "    \"\"\"\n",
    "    For each currency in currency_list, performs an API call to fetch exchange rates,\n",
    "    extracts the USD rate, and returns a DataFrame with columns: base_currency, usd_rate, timestamp.\n",
    "    Handles HTTP errors and rate limiting (429) with a 20-minute wait.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for base_currency in currency_list:\n",
    "        request_url = base_url + base_currency\n",
    "        try:\n",
    "            logger.info(f\"Requesting exchange rate for base currency: {base_currency}\")\n",
    "            response = requests.get(request_url)\n",
    "            if response.status_code == 429:\n",
    "                logger.warning(\"Rate limit hit (429). Waiting for 20 minutes before retrying...\")\n",
    "                time.sleep(20 * 60)\n",
    "                response = requests.get(request_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            usd_rate = data.get(\"rates\", {}).get(\"USD\")\n",
    "            timestamp = data.get(\"time_last_update_utc\")\n",
    "            if usd_rate is not None:\n",
    "                results.append((base_currency, float(usd_rate), timestamp))\n",
    "                logger.info(f\"Fetched USD rate for {base_currency}: {usd_rate}\")\n",
    "            else:\n",
    "                logger.warning(f\"USD rate not found for base currency: {base_currency}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"HTTP error for {base_currency}: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error for {base_currency}: {e}\")\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"base_currency\", StringType(), True),\n",
    "        StructField(\"usd_rate\", DoubleType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True)\n",
    "    ])\n",
    "    currency_exchange = spark.createDataFrame(results, schema=schema)\n",
    "    return currency_exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff4ddf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO - Needs testing and finish developing for needs\n",
    "def process_currency_conversion_and_transform(\n",
    "    api_requests_df,\n",
    "    usd_sales_data,\n",
    "    currency_exchange,\n",
    "    logger\n",
    "):\n",
    "    \"\"\"\n",
    "    Joins api_requests_df with currency_exchange to calculate USD sales amounts,\n",
    "    processes USD sales, and produces order_product_df and order_df as per schema.\n",
    "    Includes error handling and logging.\n",
    "    Returns: order_product_df, order_df\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Joining api_requests_df with currency_exchange to get USD rates...\")\n",
    "        api_requests_with_rates = api_requests_df.join(\n",
    "            currency_exchange,\n",
    "            api_requests_df[\"currency\"] == currency_exchange[\"base_currency\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        logger.info(\"Calculating sale_amount_usd for non-USD currencies...\")\n",
    "        api_requests_with_rates = api_requests_with_rates.withColumn(\n",
    "            \"sale_amount_usd\",\n",
    "            pyspark_round(col(\"sale_amount\") * col(\"usd_rate\"), 2)\n",
    "        )\n",
    "\n",
    "        api_requests_with_rates = api_requests_with_rates.withColumn(\n",
    "            \"exchange_rate\", col(\"usd_rate\").cast(\"decimal(12,6)\")\n",
    "        ).withColumn(\n",
    "            \"exchange_timestamp\", col(\"timestamp\")\n",
    "        ).withColumn(\n",
    "            \"base_currency\", col(\"currency\")\n",
    "        ).withColumn(\n",
    "            \"inserted_datetime\", current_timestamp()\n",
    "        )\n",
    "\n",
    "        logger.info(\"Processing USD sales data...\")\n",
    "        usd_sales_data_with_rates = usd_sales_data.withColumn(\n",
    "            \"sale_amount_usd\", col(\"sale_amount\")\n",
    "        ).withColumn(\n",
    "            \"exchange_rate\", lit(1).cast(\"decimal(12,6)\")\n",
    "        ).withColumn(\n",
    "            \"exchange_timestamp\", current_timestamp()\n",
    "        ).withColumn(\n",
    "            \"base_currency\", col(\"currency\")\n",
    "        ).withColumn(\n",
    "            \"inserted_datetime\", current_timestamp()\n",
    "        )\n",
    "\n",
    "        order_product_cols = [\n",
    "            \"order_product_id\", \"order_id\", \"product_id\", \"sale_amount_usd\", \"sale_amount\",\n",
    "            \"base_currency\", \"exchange_rate\", \"exchange_timestamp\", \"discount\", \"inserted_datetime\"\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Combining non-USD and USD sales into order_product_df...\")\n",
    "        order_product_df = api_requests_with_rates.select(order_product_cols).unionByName(\n",
    "            usd_sales_data_with_rates.select(order_product_cols)\n",
    "        )\n",
    "\n",
    "        logger.info(\"Aggregating order_product_df to create order_df...\")\n",
    "        order_df = order_product_df.groupBy(\n",
    "            \"order_id\", \"customer_id\", \"region\", \"order_date\"\n",
    "        ).agg(\n",
    "            pyspark_round(col(\"sale_amount_usd\").sum(), 2).alias(\"total_sales_amount_usd\")\n",
    "        ).withColumn(\n",
    "            \"inserted_datetime\", current_timestamp()\n",
    "        )\n",
    "\n",
    "        order_cols = [\n",
    "            \"order_id\", \"customer_id\", \"total_sales_amount_usd\", \"region\", \"order_date\", \"inserted_datetime\"\n",
    "        ]\n",
    "        order_df = order_df.select(order_cols)\n",
    "\n",
    "        logger.info(\"Transformation to order_product_df and order_df completed successfully.\")\n",
    "        return order_product_df, order_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during currency conversion and transformation: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f60f0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_to_database(df, table_name, logger, mode=\"overwrite\", jdbc_url=JDBC_URL, connection_properties=CONNECTION_PROPERTIES):\n",
    "    \"\"\"\n",
    "    Writes a PySpark DataFrame to the specified table in the Azure SQL Database.\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame to write.\n",
    "        table_name (str): The target table name in the database.\n",
    "        logger (Logger): Logger for logging info and errors.\n",
    "        mode (str): Write mode ('append', 'overwrite', etc.). Default is 'append'.\n",
    "        jdbc_url (str): JDBC URL for the database connection.\n",
    "        connection_properties (dict): Connection properties for JDBC.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Writing DataFrame to table '{table_name}' with mode '{mode}'...\")\n",
    "        df.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table_name,\n",
    "            mode=mode,\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        logger.info(f\"Successfully wrote DataFrame to table '{table_name}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write DataFrame to table '{table_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3de2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Run pipeline setup to get Spark session and logger\n",
    "    spark, logger = pipeline_setup()\n",
    "    # Extract data and validate, returning only data passing the\n",
    "    # data quality rules\n",
    "    validated_df, product_ref_df = validate_data(spark=spark, logger=logger)\n",
    "\n",
    "    #Â Transform the product ref data to expected data types\n",
    "    product_ref_df = transform_schema(product_ref_df\n",
    "                                      , product_ref_schema_dict)\n",
    "    \n",
    "    product_ref_df = product_ref_df.withColumn(\"inserted_datetime\"\n",
    "                                               , current_timestamp())\n",
    "    # Drop is_dup_row and is_null columns if they exist\n",
    "    for col_name in [\"is_dup_row\", \"is_null\"]:\n",
    "        if col_name in product_ref_df.columns:\n",
    "            product_ref_df = product_ref_df.drop(col_name)\n",
    "\n",
    "    # Transform the sales data to expected data types\n",
    "    sales_df = transform_schema(validated_df\n",
    "                                      , sales_schema_dict)\n",
    "    \n",
    "    for col_name in [\"is_dup_row\", \"is_null\"\n",
    "                     , \"is_orphaned_product_id\", \"ref_product_id\"]:\n",
    "        if col_name in sales_df.columns:\n",
    "            sales_df = sales_df.drop(col_name)\n",
    "\n",
    "    # Add order_product_id as concatenation of order_id and product_id\n",
    "    sales_df = sales_df.withColumn(\"order_product_id\"\n",
    "                                   , concat_ws(\"_\", col(\"order_id\")\n",
    "                                               , col(\"product_id\")))\n",
    "\n",
    "    # Split sales df into two types: currency conversion (via API) needed vs. not\n",
    "    api_requests_df = sales_df.filter(col(\"currency\") != \"usd\")\n",
    "    currency_list = [row.currency for row in api_requests_df\n",
    "                     .select(\"currency\").distinct().collect()]\n",
    "    \n",
    "    usd_sales_data = sales_df.filter(col(\"currency\") == \"usd\")\n",
    "\n",
    "\n",
    "    # Retrieve the currency rates using API calls\n",
    "    currency_exchange_df = fetch_usd_exchange_rates(spark, logger, currency_list)\n",
    "    currency_exchange_df.show()\n",
    "\n",
    "    ## TO DO: \n",
    "        # Join the api rates to the api_requests_df then join\n",
    "\n",
    "        #Â Create dataframes for order and order_product (incl. final fields\n",
    "        # such as the aggregation of total order sales)\n",
    "\n",
    "        # Load order and order_product into the database as an append\n",
    "\n",
    "    # Load the product_ref data into the database\n",
    "    # When testing complete, mode should be updated to merge\n",
    "    # During testing it is creating primary key violations\n",
    "    write_df_to_database(product_ref_df, 'product_ref', logger, mode='overwrite')\n",
    "    #Â End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be01ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 01:11:08,975 - INFO - Logger initialised and ready to use.\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/27 01:11:09 WARN Utils: Your hostname, Abbys-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.18.5 instead (on interface en0)\n",
      "25/07/27 01:11:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/27 01:11:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2025-07-27 01:11:10,930 - INFO - Initialised Spark Session\n",
      "2025-07-27 01:11:12,438 - INFO - Successfully connected to Azure SQL Database.\n",
      "2025-07-27 01:11:14,619 - INFO - Missing tables detected: ['product_ref', 'order_product']. Executing SQL script to create missing tables using PyODBC.\n",
      "2025-07-27 01:11:16,306 - INFO - Executed SQL statement for table product_ref: CREATE TABLE product_ref (\n",
      "        product_id VARCHAR(6) NOT NULL PRIMARY KEY,\n",
      "        product_name VARCHAR(100) NOT NULL,\n",
      "        product_reference VARCHAR(100) NOT NULL,\n",
      "        inserted_datetime DATETIME2 NOT NULL\n",
      "    )\n",
      "2025-07-27 01:11:16,577 - INFO - Executed SQL statement for table order_product: CREATE TABLE order_product (\n",
      "        order_product_id VARCHAR(15) NOT NULL PRIMARY KEY,\n",
      "        order_id VARCHAR(6) NOT NULL,\n",
      "        product_id VARCHAR(6) NOT NULL,\n",
      "        sale_amount_usd DECIMAL(8,2) NOT NULL,\n",
      "        sale_amount DECIMAL(8,2) NOT NULL,\n",
      "        base_currency VARCHAR(2) NOT NULL,\n",
      "        exchange_rate DECIMAL(12,6) NOT NULL,\n",
      "        exchange_timestamp DATETIME2 NOT NULL,\n",
      "        discount DECIMAL(8,2),\n",
      "        inserted_datetime DATETIME2 NOT NULL\n",
      "    )\n",
      "2025-07-27 01:11:16,578 - INFO - Pipeline setup complete.\n",
      "2025-07-27 01:11:16,579 - INFO - Loading product_ref data from file...\n",
      "2025-07-27 01:11:16,866 - INFO - Loaded data from /Users/abbywalker/Documents/coding/currency_conversion_proj/data/product_reference 2.csv as CSV.\n",
      "2025-07-27 01:11:16,881 - INFO - Flagged all rows with NULLs which violate exceptions.\n",
      "2025-07-27 01:11:16,923 - INFO - Flagged all rows which are duplicates.\n",
      "2025-07-27 01:11:16,932 - INFO - product_ref data loaded and processed from file.\n",
      "2025-07-27 01:11:16,932 - INFO - Loading sales_data...\n",
      "2025-07-27 01:11:17,011 - INFO - Loaded data from /Users/abbywalker/Documents/coding/currency_conversion_proj/data/sales_data 2.csv as CSV.\n",
      "2025-07-27 01:11:17,028 - INFO - Flagged all rows with NULLs which violate exceptions.\n",
      "2025-07-27 01:11:17,049 - INFO - Flagged all rows which are duplicates.\n",
      "2025-07-27 01:11:17,052 - INFO - sales_data loaded and processed.\n",
      "2025-07-27 01:11:17,052 - INFO - Joining sales_data to product_ref to check for orphaned product_ids...\n",
      "2025-07-27 01:11:17,071 - INFO - Join complete. Flagged orphaned product_ids.\n",
      "2025-07-27 01:11:17,579 - INFO - Total rows: 20, Poor quality rows: 4 (20.00%)\n",
      "2025-07-27 01:11:21,212 - INFO - Data quality is within acceptable limits of 25%.\n",
      "2025-07-27 01:11:21,497 - INFO - Requesting exchange rate for base currency: GBP\n",
      "2025-07-27 01:11:21,546 - INFO - Fetched USD rate for GBP: 1.343698\n",
      "2025-07-27 01:11:21,547 - INFO - Requesting exchange rate for base currency: EUR\n",
      "2025-07-27 01:11:21,588 - INFO - Fetched USD rate for EUR: 1.17401\n",
      "2025-07-27 01:11:21,588 - INFO - Requesting exchange rate for base currency: USD\n",
      "2025-07-27 01:11:21,626 - INFO - Fetched USD rate for USD: 1\n",
      "2025-07-27 01:11:22,080 - INFO - Writing DataFrame to table 'product_ref' with mode 'overwrite'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+--------------------+\n",
      "|base_currency|usd_rate|           timestamp|\n",
      "+-------------+--------+--------------------+\n",
      "|          GBP|1.343698|Sat, 26 Jul 2025 ...|\n",
      "|          EUR| 1.17401|Sat, 26 Jul 2025 ...|\n",
      "|          USD|     1.0|Sat, 26 Jul 2025 ...|\n",
      "+-------------+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 01:11:25,912 - INFO - Successfully wrote DataFrame to table 'product_ref'.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d3eb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac021c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
